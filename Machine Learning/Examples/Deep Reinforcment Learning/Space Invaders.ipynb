{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heated-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-sleep",
   "metadata": {},
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bridal-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "still-consensus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "considerable-psychology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Score:135.0\n",
      "Episode:1 Score:155.0\n",
      "Episode:2 Score:105.0\n",
      "Episode:3 Score:210.0\n",
      "Episode:4 Score:105.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-hindu",
   "metadata": {},
   "source": [
    "# 2. Create a Deep learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "driven-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equipped-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height,width,channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.__len__=actions\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "female-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specialized-kinase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               128250    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1506      \n",
      "=================================================================\n",
      "Total params: 34,809,212\n",
      "Trainable params: 34,809,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-romance",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "metallic-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "expensive-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg',\n",
    "                  nb_actions=actions, nb_steps_warmup=1000)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dangerous-mambo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mclea\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  603/10000: episode: 1, duration: 3.609s, episode steps: 603, steps per second: 167, episode reward: 105.000, mean reward:  0.174 [ 0.000, 25.000], mean action: 2.517 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mclea\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1129/10000: episode: 2, duration: 17.507s, episode steps: 526, steps per second:  30, episode reward: 30.000, mean reward:  0.057 [ 0.000, 15.000], mean action: 2.563 [0.000, 5.000],  loss: 18.005289, mean_q: 6.854127, mean_eps: 0.904195\n",
      " 1866/10000: episode: 3, duration: 73.146s, episode steps: 737, steps per second:  10, episode reward: 135.000, mean reward:  0.183 [ 0.000, 25.000], mean action: 2.498 [0.000, 5.000],  loss: 0.658710, mean_q: 5.172617, mean_eps: 0.865270\n",
      " 2806/10000: episode: 4, duration: 95.532s, episode steps: 940, steps per second:  10, episode reward: 170.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.296 [0.000, 5.000],  loss: 0.811811, mean_q: 5.534914, mean_eps: 0.789805\n",
      " 3685/10000: episode: 5, duration: 88.947s, episode steps: 879, steps per second:  10, episode reward: 285.000, mean reward:  0.324 [ 0.000, 30.000], mean action: 2.268 [0.000, 5.000],  loss: 0.771942, mean_q: 4.554386, mean_eps: 0.707950\n",
      " 4773/10000: episode: 6, duration: 108.839s, episode steps: 1088, steps per second:  10, episode reward: 310.000, mean reward:  0.285 [ 0.000, 30.000], mean action: 2.560 [0.000, 5.000],  loss: 0.546630, mean_q: 5.222404, mean_eps: 0.619435\n",
      " 5454/10000: episode: 7, duration: 65.418s, episode steps: 681, steps per second:  10, episode reward: 120.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.217 [0.000, 5.000],  loss: 0.332122, mean_q: 4.824737, mean_eps: 0.539830\n",
      " 6097/10000: episode: 8, duration: 66.739s, episode steps: 643, steps per second:  10, episode reward: 120.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.118 [0.000, 5.000],  loss: 0.238894, mean_q: 4.487463, mean_eps: 0.480250\n",
      " 6670/10000: episode: 9, duration: 62.548s, episode steps: 573, steps per second:   9, episode reward: 30.000, mean reward:  0.052 [ 0.000, 10.000], mean action: 2.785 [0.000, 5.000],  loss: 0.080411, mean_q: 4.570923, mean_eps: 0.425530\n",
      " 7065/10000: episode: 10, duration: 44.040s, episode steps: 395, steps per second:   9, episode reward: 75.000, mean reward:  0.190 [ 0.000, 25.000], mean action: 2.443 [0.000, 5.000],  loss: 0.151665, mean_q: 4.817702, mean_eps: 0.381970\n",
      " 7681/10000: episode: 11, duration: 62.998s, episode steps: 616, steps per second:  10, episode reward: 110.000, mean reward:  0.179 [ 0.000, 25.000], mean action: 2.631 [0.000, 5.000],  loss: 0.184290, mean_q: 4.996361, mean_eps: 0.336475\n",
      " 8598/10000: episode: 12, duration: 98.089s, episode steps: 917, steps per second:   9, episode reward: 270.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.486 [0.000, 5.000],  loss: 0.344662, mean_q: 4.995819, mean_eps: 0.267490\n",
      " 9054/10000: episode: 13, duration: 45.801s, episode steps: 456, steps per second:  10, episode reward: 100.000, mean reward:  0.219 [ 0.000, 20.000], mean action: 3.026 [0.000, 5.000],  loss: 0.261678, mean_q: 4.903106, mean_eps: 0.205705\n",
      " 9558/10000: episode: 14, duration: 55.310s, episode steps: 504, steps per second:   9, episode reward: 95.000, mean reward:  0.188 [ 0.000, 20.000], mean action: 3.042 [0.000, 5.000],  loss: 0.145139, mean_q: 4.684855, mean_eps: 0.162505\n",
      "done, took 939.518 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d67243ac88>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "thrown-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 110.000, steps: 728\n",
      "Episode 2: reward: 335.000, steps: 798\n",
      "Episode 3: reward: 120.000, steps: 818\n",
      "Episode 4: reward: 80.000, steps: 503\n",
      "Episode 5: reward: 150.000, steps: 958\n",
      "Episode 6: reward: 135.000, steps: 770\n",
      "Episode 7: reward: 135.000, steps: 634\n",
      "Episode 8: reward: 105.000, steps: 777\n",
      "Episode 9: reward: 105.000, steps: 515\n",
      "Episode 10: reward: 85.000, steps: 610\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-fb33c37a9e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"episode_reward\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history([\"episode_reward\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-impact",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
